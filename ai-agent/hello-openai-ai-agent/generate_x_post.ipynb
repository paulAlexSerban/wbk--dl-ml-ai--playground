{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL_AI=\"gpt-4.1-nano\"\n",
    "MAX_TOKENS=100\n",
    "\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_x_post(prompt:str, system_prompt: str, model:str=MODEL_AI, max_tokens:int=MAX_TOKENS) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using OpenAI's API.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt for text generation.\n",
    "        system_prompt (str): The system prompt to guide the generation.\n",
    "        model (str): The model to use for generation.\n",
    "        max_tokens (int): The maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b926d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(topic: str) -> str:\n",
    "    with open(\"post_examples.json\", \"r\") as f:\n",
    "        examples = json.load(f)\n",
    "    examples_str = \"\"\n",
    "\n",
    "    for i, example in enumerate(examples):\n",
    "        examples_str += f\"\"\"\n",
    "        <example-{i+1}>\n",
    "            <topic>\n",
    "                {example['topic']}\n",
    "            </topic>\n",
    "            <generate-post>\n",
    "                {example['post']}\n",
    "            </generate-post>\n",
    "        </example-{i+1}>\n",
    "        \"\"\"\n",
    "\n",
    "    prompt=f\"\"\"\n",
    "        You are an expert social media manager, and you excel at crafting viral and highly engaging posts for X (formerly Twitter).\n",
    "\n",
    "        Your task is to generate a post that is concise, impactful, and tailored to the topic provided by the user.\n",
    "\n",
    "        Avoid using hashtags and lots of emojis (a few emojis are okay, but not too many).\n",
    "\n",
    "        Keep the post short and focused, structure it in a clean, readable way, using line breaks and empty lines to enhance readability.\n",
    "        The topic for the post is:\n",
    "        <topic>\n",
    "            {topic}\n",
    "        </topic>\n",
    "\n",
    "        <examples>\n",
    "            {examples_str}\n",
    "        </examples>\n",
    "\n",
    "        Use the tone, language, structure and style of the examples above to generate a post that is engaging and relevant to the topic provided by the user.\n",
    "        DO NOT USE THE CONTENT FROM THE EXAMPLES!\n",
    "        \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"When Lambdas should be grouped in a service and when they should be separate?\"\n",
    "llm_response= generate_x_post(prompt=get_prompt(topic), system_prompt=\"Generate a viral and engaging post for X (formerly Twitter) about the topic provided.\")\n",
    "\n",
    "# write llm reponse to file\n",
    "with open(f\"responses/generated_post_{topic.replace(' ', '_')}.txt\", \"w\") as f:    f.write(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
