{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "sentence = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy. There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which don't look even slightly believable. If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text. All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet. It uses a dictionary of over 200 Latin words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable. The generated Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).\"\n",
    "# lowercase all the words\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase each word in the sentence - this is needed because words have to be lowercase to be compared to the stopwords\n",
    "lower_case_sentence = lower_case(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove STOP Words\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentence, stop_words):\n",
    "    return ' '.join([word for word in sentence.split() if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words(\"english\")\n",
    "stopwords_en.remove(\"not\")\n",
    "stopwords_en.append(\"python\")\n",
    "\n",
    "sentence_no_stopwords = remove_stop_words(lower_case_sentence, stopwords_en)\n",
    "print(sentence_no_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "- process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenize = sent_tokenize(sentence_no_stopwords)\n",
    "print(sentence_tokenize)\n",
    "\n",
    "word_tokenize = word_tokenize(sentence_no_stopwords)\n",
    "print(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no_punctuation = remove_punctuation(sentence_no_stopwords)\n",
    "print(sentence_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "- Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "connect_tokens = [\"connecting\", \"connected\", \"connection\", \"connects\", \"connectivity\"]\n",
    "\n",
    "# loop and stem each individual token\n",
    "for token in connect_tokens:\n",
    "    print(token, \" : \", ps.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_tokens = [\"learning\", \"learned\", \"learns\", \"learnable\", \"learner\", \"learners\", \"learnings\"]\n",
    "for token in learn_tokens:\n",
    "    print(token, \" : \", ps.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_tokens = [\"liking\", \"liked\", \"likes\", \"likable\", \"liker\", \"likers\", \"likings\"]\n",
    "for token in like_tokens:\n",
    "    print(token, \" : \", ps.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_tokens = [\"likes\", \"better\", \"worse\"]\n",
    "for token in likes_tokens:\n",
    "    print(token, \" : \", ps.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "- Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for token in connect_tokens:\n",
    "    print(token, \" : \", lemmatizer.lemmatize(token, pos=\"v\"))\n",
    "    \n",
    "for token in learn_tokens:\n",
    "    print(token, \" : \", lemmatizer.lemmatize(token, pos=\"v\"))\n",
    "    \n",
    "for token in like_tokens:\n",
    "    print(token, \" : \", lemmatizer.lemmatize(token, pos=\"v\"))\n",
    "    \n",
    "for token in likes_tokens:\n",
    "    print(token, \" : \", lemmatizer.lemmatize(token, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "- process of breaking the text down into a sequence of n-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokens = sentence_no_punctuation.split()\n",
    "\n",
    "unigrams = pd.Series(nltk.ngrams(tokens, 1)).value_counts()\n",
    "unigrams = unigrams[:10]\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams[:10].sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = pd.Series(nltk.ngrams(tokens, 2)).value_counts()\n",
    "bigrams = bigrams[:10]\n",
    "print(bigrams)\n",
    "\n",
    "bigrams[:10].sort_values().plot(kind='barh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
